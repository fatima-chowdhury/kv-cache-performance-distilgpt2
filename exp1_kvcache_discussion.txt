Discussion (Experiment 1 â€” KV-Cache):
- Device: cpu
- With KV-cache, throughput changed by ~214.7% vs. no-cache.
- Total latency changed by ~185.5% (positive means latency improved with cache).
- Peak memory changed by ~-99.8% (positive means more memory with cache).
These trends match the expectation that KV-cache speeds up autoregressive decoding by reusing past keys/values,
often increasing throughput and reducing per-token latency, at the cost of extra memory to store the cache.